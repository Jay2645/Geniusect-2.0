[Rewards]
FaintedReward: 5
HPReward: 4
StatusReward: 3
ReferenceValue: 0
VictoryReward: 10

[Train]
NumTrainingSteps: 500000
NumEvaluationEpisodes: 100
DropoutKeepInputLayer: 0.9
DropoutKeepHiddenLayer: 0.8
# Options:
# Default: Chooses the 0th action Showdown gives, every turn. Surprisingly good; results between Random and Max AI on average
# Random: Does something random every turn. ~50% winrate againt untrained model (which makes intuitive sense)
# Max: Goes for the move that does the most damage every turn. Will never set up, switch, or use hazards. Harder for AI to learn
# Cycle: Cycles through Default -> Random -> Max -> Default -> etc.
# Ladder: Challenges the Showdown ladder
# Capitalization does not matter
Opponent: Default

[DQN]
NumberWarmupSteps: 500
Gamma: 0.5
TargetModelUpdate: 1
# Delta used for Huber Loss function, used by the DQNAgent to measure loss
# See https://www.machinecurve.com/index.php/2019/10/12/using-huber-loss-in-keras/
DeltaClip: 1.5
UseDoubleDQN: True

[Saving]
CheckpointDir: checkpoint
UseCheckpoint: True
AutoLoadFromCheckpoint: True

[Execution]
StepTimeout: 60.0
