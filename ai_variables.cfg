[Rewards]
FaintedReward: 2
HPReward: 15
StatusReward: 5
ReferenceValue: 0
VictoryReward: 1

[Train]
NumTrainingSteps: 500000
NumEvaluationEpisodes: 100
DropoutKeepInputLayer: 0.85
DropoutKeepHiddenLayer: 0.5
# How fast we learn. 
# If this is too big, it will diverge and fail to find the minimum of our loss function, creating a sine wave as it tries and fails to find the minimum
# Too small, and our model may flatten out and get stuck
# Most important parameter to tweak -- best approach for testing is to use a logarithmic scale (i.e. {0.1, 0.01, 0.001, etc.})
LearningRate: 0.1
# The exponential decay rate for the 1st moment estimates. 
Beta1: 0.9
# The exponential decay rate for the 2nd moment estimates.
Beta2: 0.999
# A small constant for numerical stability. 
# The default value of 0.0000001 for epsilon might not be a good default in general.
# For example, when training an Inception network on ImageNet a current good choice is 1.0 or 0.1. 
Epsilon: 0.0000001
# Whether to apply AMSGrad variant of this algorithm from the paper "On the Convergence of Adam and beyond". 
AMSGrad: False

[Opponent]
# Options:
# Default: Chooses the 0th action Showdown gives, every turn. Surprisingly good; results between Random and Max AI on average
# Random: Does something random every turn. ~50% winrate againt untrained model (which makes intuitive sense)
# Max: Goes for the move that does the most damage every turn. Will never set up, switch, or use hazards. Harder for AI to learn
# Heuristics: Uses a heuristic-based AI
# Cycle: Cycles through Default -> Random -> Max -> Heuristics -> Default -> etc.
# Self: Battles against the last checkpoint this AI did
# Ladder: Challenges the Showdown ladder
# Capitalization does not matter
Opponent: Heuristics
StartingTryhard: 0.9
TryhardFloor: 0.85

[DQN]
NumberWarmupSteps: 1000
# How much we bias future rewards compared to current ones
# A gamma above 1 means future rewards are more important than something in front of us right now
Gamma: 0.98
TargetModelUpdate: 1
# Delta used for Huber Loss function, used by the DQNAgent to measure loss
# See https://www.machinecurve.com/index.php/2019/10/12/using-huber-loss-in-keras/
DeltaClip: 2.0
UseDoubleDQN: True

[Saving]
CheckpointDir: models
UseCheckpoint: True
AutoLoadFromCheckpoint: True

[Execution]
StepTimeout: 181.0
